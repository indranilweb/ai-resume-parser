import os
import json
import hashlib
import numpy as np
import pickle
import google.generativeai as genai
import pypdf  # For reading PDF files
import docx   # For reading DOCX files
import config
from sentence_transformers import SentenceTransformer
import faiss

# --- Configuration and Setup ---
GEMINI_KEY = config.GEMINI_KEY
GEMINI_MODEL = config.GEMINI_MODEL
CACHE_DIR = "cache_dir"
VECTOR_DB_DIR = "vector_db"

# Vector search configuration
ENABLE_VECTOR_SEARCH = True  # Set to False to disable vector search
SIMILARITY_THRESHOLD = 0.3   # Minimum similarity score (0-1)
MAX_VECTOR_RESULTS = None    # Maximum resumes to pass to Gemini (None = no limit)

# Ensure directories exist
for dir_path in [CACHE_DIR, VECTOR_DB_DIR]:
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)

# Initialize the sentence transformer model for embeddings
embedding_model = None
if ENABLE_VECTOR_SEARCH:
    try:
        embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        print("ğŸ”§ Sentence transformer model loaded successfully")
    except Exception as e:
        print(f"âš ï¸ Warning: Could not load sentence transformer model: {e}")
        print("Vector search will be disabled")
else:
    print("ğŸ”§ Vector search disabled by configuration")

# Initialize the Gemini client
# It automatically picks up the API key from the GEMINI_API_KEY environment variable.
try:
    # IMPORTANT: It is recommended to use environment variables for API keys.
    # For this example, the key is placed directly.
    api_key = GEMINI_KEY # os.environ.get("GEMINI_API_KEY")
    if not api_key:
        raise ValueError("GEMINI_API_KEY not found. Please set it as an environment variable or paste it here.")
    genai.configure(api_key=api_key)
except Exception as e:
    print(f"ğŸš¨ Error initializing Gemini client: {e}")
    exit()

# --- Text Extraction Functions (Unchanged) ---

def read_pdf(file_path: str) -> str:
    """Extracts text from a PDF file."""
    try:
        with open(file_path, 'rb') as file:
            reader = pypdf.PdfReader(file)
            text = ""
            for page in reader.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
            return text
    except Exception as e:
        print(f"ğŸ“„âŒ Error reading PDF file '{os.path.basename(file_path)}': {e}")
        return ""

def read_docx(file_path: str) -> str:
    """Extracts text from a DOCX file."""
    try:
        doc = docx.Document(file_path)
        text = "\n".join([para.text for para in doc.paragraphs])
        return text
    except Exception as e:
        print(f"ğŸ“„âŒ Error reading DOCX file '{os.path.basename(file_path)}': {e}")
        return ""

def read_txt(file_path: str) -> str:
    """Reads a plain text file."""
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            return file.read()
    except Exception as e:
        print(f"ğŸ“„âŒ Error reading TXT file '{os.path.basename(file_path)}': {e}")
        return ""

def get_resume_content(file_path: str) -> str:
    """
    Reads the content of a resume file by dispatching to the correct reader
    based on the file extension.
    """
    filename = os.path.basename(file_path)
    _, extension = os.path.splitext(filename)
    extension = extension.lower()

    if extension == '.txt':
        return read_txt(file_path)
    elif extension == '.pdf':
        return read_pdf(file_path)
    elif extension == '.docx':
        return read_docx(file_path)
    elif extension == '.doc':
        print(f"âš ï¸ Warning: .doc files are not directly supported. Please convert '{filename}' to .docx or .pdf.")
        return ""
    else:
        print(f"âš ï¸ Warning: Unsupported file type '{extension}' for file '{filename}'. Skipping.")
        return ""

# --- Vector Database Functions ---

def get_vector_db_path(resumes_data: dict) -> str:
    """Generate a unique vector database path based on resume content."""
    content_parts = []
    for filename, content in sorted(resumes_data.items()):
        content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()[:8]
        content_parts.append(f"{filename}:{content_hash}")
    
    combined = "|".join(content_parts)
    db_hash = hashlib.md5(combined.encode('utf-8')).hexdigest()
    return os.path.join(VECTOR_DB_DIR, f"resume_db_{db_hash}")

def create_vector_database(resumes_data: dict) -> tuple:
    """Create FAISS vector database from resume content."""
    if not embedding_model:
        return None, None
    
    db_path = get_vector_db_path(resumes_data)
    
    # Check if vector DB already exists
    if os.path.exists(f"{db_path}.index") and os.path.exists(f"{db_path}_metadata.pkl"):
        try:
            # Load existing vector database
            index = faiss.read_index(f"{db_path}.index")
            with open(f"{db_path}_metadata.pkl", 'rb') as f:
                metadata = pickle.load(f)
            print(f"ğŸ“‚ Loaded existing vector database: {os.path.basename(db_path)}")
            return index, metadata
        except Exception as e:
            print(f"âš ï¸ Could not load existing vector DB: {e}")
    
    # Create new vector database
    print("ğŸ”§ Creating vector database from resumes...")
    
    texts = []
    metadata = []
    
    for filename, content in resumes_data.items():
        # Split content into chunks for better semantic search
        chunks = split_text_into_chunks(content, chunk_size=512, overlap=50)
        for i, chunk in enumerate(chunks):
            texts.append(chunk)
            metadata.append({
                'filename': filename,
                'chunk_id': i,
                'content': chunk
            })
    
    if not texts:
        print("âŒ No text content to vectorize")
        return None, None
    
    # Generate embeddings
    print(f"ğŸ”§ Generating embeddings for {len(texts)} text chunks...")
    embeddings = embedding_model.encode(texts, show_progress_bar=False)
    
    # Create FAISS index
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity
    
    # Normalize embeddings for cosine similarity
    faiss.normalize_L2(embeddings)
    index.add(embeddings.astype(np.float32))
    
    # Save vector database
    try:
        faiss.write_index(index, f"{db_path}.index")
        with open(f"{db_path}_metadata.pkl", 'wb') as f:
            pickle.dump(metadata, f)
        print(f"ğŸ’¾ Vector database saved: {os.path.basename(db_path)}")
    except Exception as e:
        print(f"âš ï¸ Could not save vector DB: {e}")
    
    return index, metadata

def split_text_into_chunks(text: str, chunk_size: int = 512, overlap: int = 50) -> list:
    """Split text into overlapping chunks for better semantic search."""
    words = text.split()
    chunks = []
    
    for i in range(0, len(words), chunk_size - overlap):
        chunk = ' '.join(words[i:i + chunk_size])
        if chunk.strip():
            chunks.append(chunk)
    
    return chunks if chunks else [text]

def semantic_search_resumes(required_skills: list, resumes_data: dict, top_k: int = None, similarity_threshold: float = None) -> dict:
    """Perform semantic search to filter resumes based on required skills."""
    if not embedding_model:
        print("âš ï¸ Vector search disabled - returning all resumes")
        return resumes_data
    
    if not required_skills or not resumes_data:
        return resumes_data
    
    # Use global configuration if not specified
    if top_k is None:
        top_k = MAX_VECTOR_RESULTS
    if similarity_threshold is None:
        similarity_threshold = SIMILARITY_THRESHOLD
    
    # Create or load vector database
    index, metadata = create_vector_database(resumes_data)
    if not index or not metadata:
        print("âŒ Could not create vector database - returning all resumes")
        return resumes_data
    
    # Create query from required skills
    skills_query = f"Required skills and experience: {', '.join(required_skills)}"
    
    print(f"ğŸ” Performing semantic search for: {skills_query}")
    
    # Generate query embedding
    query_embedding = embedding_model.encode([skills_query])
    faiss.normalize_L2(query_embedding)
    
    # Search for similar chunks
    search_k = min(len(metadata), top_k if top_k else len(metadata))
    scores, indices = index.search(query_embedding.astype(np.float32), search_k)
    
    # Aggregate scores per resume file
    resume_scores = {}
    for score, idx in zip(scores[0], indices[0]):
        if score >= similarity_threshold:
            filename = metadata[idx]['filename']
            if filename not in resume_scores:
                resume_scores[filename] = []
            resume_scores[filename].append(score)
    
    # Calculate average score per resume
    filtered_resumes = {}
    for filename, scores_list in resume_scores.items():
        avg_score = sum(scores_list) / len(scores_list)
        if avg_score >= similarity_threshold:
            filtered_resumes[filename] = resumes_data[filename]
            print(f"  âœ… {filename} (similarity: {avg_score:.3f})")
    
    if filtered_resumes:
        print(f"ğŸ¯ Vector search filtered {len(resumes_data)} â†’ {len(filtered_resumes)} resumes")
        return filtered_resumes
    else:
        print(f"âš ï¸ No resumes met similarity threshold ({similarity_threshold}) - returning all resumes")
        return resumes_data

# --- Cache Management Functions ---

def generate_cache_key(resumes_data: dict, required_skills: list[str]) -> str:
    """Generate a unique cache key based on resume content and skills."""
    # Create a combined string from resume filenames, content hashes, and skills
    content_parts = []
    for filename, content in sorted(resumes_data.items()):
        content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()[:8]
        content_parts.append(f"{filename}:{content_hash}")
    
    skills_str = ",".join(sorted([skill.strip().lower() for skill in required_skills]))
    combined = "|".join(content_parts) + f"|skills:{skills_str}"
    
    # Add vector search indicator to cache key
    vector_indicator = "vector_enabled" if embedding_model else "vector_disabled"
    combined += f"|{vector_indicator}"
    
    return hashlib.md5(combined.encode('utf-8')).hexdigest()

def get_cached_result(cache_key: str) -> list[dict]:
    """Retrieve cached result if it exists."""
    cache_file = os.path.join(CACHE_DIR, f"{cache_key}.json")
    if os.path.exists(cache_file):
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                result = json.load(f)
            print(f"ğŸ“‚ Cache file found: {cache_key[:12]}...json")
            return result
        except Exception as e:
            print(f"âš ï¸  Warning: Could not read cache file: {e}")
    return None

def save_to_cache(cache_key: str, result: list[dict]) -> None:
    """Save result to cache."""
    cache_file = os.path.join(CACHE_DIR, f"{cache_key}.json")
    try:
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ Cache file created: {cache_key[:12]}...json")
    except Exception as e:
        print(f"âš ï¸  Warning: Could not save to cache: {e}")

# --- Gemini API Interaction (Modified for Batch Processing) ---

def construct_batch_prompt(resumes_data: dict, required_skills: list[str]) -> str:
    """
    Constructs a detailed prompt for the Gemini API to process multiple resumes,
    filter them by skills, and extract data for the matched ones.
    """
    skills_string = ", ".join(required_skills)

    # Combine all resume texts into one block, with clear separators
    combined_resume_text = ""
    for filename, text in resumes_data.items():
        combined_resume_text += f"--- START OF RESUME: {filename} ---\n"
        combined_resume_text += text + "\n"
        combined_resume_text += f"--- END OF RESUME: {filename} ---\n\n"

    # The prompt is carefully structured to guide the model.
    # It first asks the model to filter and then extract.
    prompt = f"""
    You are an expert HR recruitment assistant. Your task is to analyze a batch of resumes, identify candidates who match a specific set of skills, and then extract key information for ONLY the matched candidates in a strict JSON format.

    The required technical skills we are looking for are: {skills_string}.

    Below is a collection of resumes. Each resume is clearly marked with its source filename.
    --- BATCH OF RESUMES START ---
    {combined_resume_text}
    --- BATCH OF RESUMES END ---

    **Your Instructions:**

    1.  **Analyze and Filter:** Carefully read every resume provided in the batch above. Identify which resumes are a strong match for the required skills: "{skills_string}". A strong match means the resume explicitly mentions several of these skills.

    2.  **Extract Information for Matched Resumes ONLY:** For each resume that you identified as a strong match, extract the following information and format it as a JSON object. Adhere strictly to the data types and formats specified:
        * "source_file": (String) The original filename of the resume (provided in the start/end markers).
        * "name": (String) The full name of the candidate.
        * "contact_number": (String) The primary phone number.
        * "last_3_companies": (Array of Strings) A list of the last 3 companies the candidate worked for, starting with the most recent. Crucially, include only official company names. Exclude project names, client names, or internal divisions within a company. If fewer than 3 companies are clearly stated, include only all available.
        * "top_5_technical_skills": (Array of Strings) A list of up to 5 technical skills explicitly mentioned in the resume that are most directly relevant to the {skills_string} and/or are highly prominent in the candidate's experience.
        * "years_of_experience": (Number) The total calculated professional work experience in years. Calculate this based on the start and end dates of all full-time work experiences listed. Round it to the nearest whole number
        * "summary": (String) A concise summary of the candidate's professional background and expertise, and most significant achievements. This must be no more than 100 words.

    **Output Format:**
    Your output MUST be a single, valid JSON array `[]` containing one JSON object for each matched candidate. If no candidates match the required skills, you MUST return an empty array `[]`.
    Do not include any explanations, introductory text, markdown formatting like ```json, or any text outside of the final JSON array.
    If a piece of information cannot be found, use `null` as the value for that key.
    """
    print("ğŸ“ Constructed a batch prompt for the Gemini API.")
    return prompt

def parse_resumes_batch(resumes_data: dict, required_skills: list[str]) -> list[dict]:
    """Sends the combined resume text to the Gemini API for batch parsing and filtering."""
    if not resumes_data:
        print("âŒ No resume content to process.")
        return []

    # Check cache first
    cache_key = generate_cache_key(resumes_data, required_skills)
    print(f"ğŸ”‘ Generated cache key: {cache_key[:12]}...")
    cached_result = get_cached_result(cache_key)
    
    if cached_result is not None:
        print("ğŸ¯ CACHE HIT: Found cached result! Skipping Gemini API call.")
        print(f"âœ… Returning {len(cached_result)} cached candidate(s)")
        return cached_result

    print("âŒ CACHE MISS: No cached result found.")
    prompt = construct_batch_prompt(resumes_data, required_skills)
    print("ğŸš€ GEMINI API: Connecting to process the batch... (This may take a moment)")

    try:
        # Select the model, Gemini 1.5 Flash is good for this task.
        model = genai.GenerativeModel(GEMINI_MODEL)

        # Configure the generation to output JSON
        generation_config = genai.types.GenerationConfig(
            temperature=0.2,
            response_mime_type="application/json"
        )

        # Call the API with the single, combined prompt
        response = model.generate_content(prompt, generation_config=generation_config)
        
        # The response text should be a valid JSON string (a list of objects)
        response_content = response.text
        parsed_data = json.loads(response_content)
        
        # Save to cache
        save_to_cache(cache_key, parsed_data)
        print("ğŸ’¾ CACHE SAVE: Result saved to cache for future use.")
        print(f"âœ… Gemini API returned {len(parsed_data)} candidate(s)")
        
        return parsed_data
    except json.JSONDecodeError:
        print("\nğŸš¨ Error: Failed to decode JSON from the API response.")
        print(f"Raw Gemini Response:\n---\n{response.text}\n---")
        return []
    except Exception as e:
        print(f"\nğŸš¨ An error occurred while communicating with the Gemini API: {e}")
        if 'response' in locals() and hasattr(response, 'text'):
            print(f"Raw Gemini Response:\n---\n{response.text}\n---")
        return []

# --- Main Application Logic (Modified for Batch Processing) ---
class ResumeParser:
    def main(self, dir_path: str, query_string: str) -> list[dict]:
        """Main function to run the resume parser application."""
        print("ğŸ¤– --- AI-Powered Resume Parser (Vector + Batch Mode) ---")
        
        resume_dir = dir_path.strip()
        if not os.path.isdir(resume_dir):
            print(f"âŒ Error: Directory '{resume_dir}' not found.")
            return []
            
        skills_input = query_string.strip()
        if not skills_input:
            print("âŒ Error: You must specify at least one skill.")
            return []
        required_skills = [skill.strip() for skill in skills_input.split(',')]

        supported_extensions = ('.txt', '.pdf', '.docx', '.doc')
        resume_files = [f for f in os.listdir(resume_dir) if f.lower().endswith(supported_extensions)]

        if not resume_files:
            print(f"âŒ No supported resumes (.txt, .pdf, .docx) found in '{resume_dir}'.")
            return []

        print(f"\nğŸ“‚ Found {len(resume_files)} resume(s). Reading content...")
        
        all_resumes_data = {}
        for filename in resume_files:
            file_path = os.path.join(resume_dir, filename)
            resume_text = get_resume_content(file_path)
            if resume_text and resume_text.strip():
                all_resumes_data[filename] = resume_text
                print(f"  âœ… Successfully read '{filename}'")
            else:
                print(f"  âŒ Could not read content from '{filename}'. Skipping.")

        if not all_resumes_data:
            print("\nâŒ Could not read any resume content. Exiting.")
            return []

        # Perform semantic search to filter resumes before Gemini API call
        print(f"\nğŸ” --- Semantic Filtering Phase ---")
        filtered_resumes = semantic_search_resumes(required_skills, all_resumes_data)
        
        if not filtered_resumes:
            print("\nâŒ --- No candidates found through semantic search. ---")
            return []
        
        if len(filtered_resumes) < len(all_resumes_data):
            print(f"ğŸ“Š Semantic search reduced API load: {len(all_resumes_data)} â†’ {len(filtered_resumes)} resumes")

        # The single API call happens here with filtered resumes
        print(f"\nğŸš€ --- Gemini API Processing Phase ---")
        matched_candidates = parse_resumes_batch(filtered_resumes, required_skills)

        if matched_candidates:
            print(f"\n\nğŸ‰ --- Found {len(matched_candidates)} Matched Candidate(s) ---")
            # Pretty-print the final JSON output
            print(json.dumps(matched_candidates, indent=4))
            return matched_candidates
        else:
            print("\nâŒ --- No candidates matched the required skills from the filtered resumes. ---")
            return []

# Example of how to run the class
if __name__ == '__main__':
    # Create a 'resumes' directory in the same folder as this script
    # and add your .pdf, .docx, or .txt resume files there.
    
    # Create an instance of the parser
    parser = ResumeParser()
    
    # Specify the directory containing resumes
    resume_directory = "resumes" 
    
    # Specify the skills you're looking for
    required_skills_query = "Python, data analysis, machine learning, SQL"
    
    # Run the main process
    # This will print the results to the console
    parser.main(resume_directory, required_skills_query)
